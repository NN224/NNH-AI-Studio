# âœ… [COMPLETED] CRITICAL FIX: AI Endpoints Ø¨Ø¯ÙˆÙ† Rate Limiting

> âš ï¸ **Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡:** Ø§Ù‚Ø±Ø£ `AI_AGENT_START_HERE.md` Ø£ÙˆÙ„Ø§Ù‹! Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù ÙƒØ§Ù…Ù„Ø§Ù‹ Ù‚Ø¨Ù„ Ø£ÙŠ ØªØ¹Ø¯ÙŠÙ„.

> **ØªÙ… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„** âœ… - Applied on Nov 30, 2025
> **Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª:**
>
> - Ø¥Ù†Ø´Ø§Ø¡ `lib/security/ai-rate-limit.ts` Ù…Ø¹ tier-based limits
> - Ø¥Ù†Ø´Ø§Ø¡ `lib/api/with-ai-protection.ts` HOF
> - ØªØ­Ø¯ÙŠØ« `/api/ai/chat` Ùˆ `/api/ai/generate` routes
> - Rate limiting Ù„ÙƒÙ„ user ÙˆÙ„ÙƒÙ„ endpoint type
> - FAIL CLOSED Ø¥Ø°Ø§ Redis ØºÙŠØ± Ù…ØªÙˆÙØ± (Ø­Ù…Ø§ÙŠØ© Ø§Ù„ØªÙƒÙ„ÙØ©)

> **Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©:** P0 - Ø­Ø±Ø¬
> **Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ù‚Ø¯Ø±:** 3 Ø³Ø§Ø¹Ø§Øª
> **Ø§Ù„Ù…Ø¬Ø§Ù„:** Ø£Ù…Ø§Ù† + ØªÙƒÙ„ÙØ©
> **Ø§Ù„Ø­Ø§Ù„Ø©:** âœ… ØªÙ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­

---

## ğŸ“‹ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©

**Issue ID:** CRITICAL-013
**Severity:** ğŸ”´ CRITICAL - COST & SECURITY
**Impact:** ØªÙƒØ§Ù„ÙŠÙ AI ØºÙŠØ± Ù…Ø­Ø¯ÙˆØ¯Ø© + Ø¥Ø³Ø§Ø¡Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù…

---

## ğŸ¯ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„

Ø§Ù„Ù€ AI endpoints (`/api/ai/*`) Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ rate limiting:

1. Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ø­Ø¯ ÙŠÙ…ÙƒÙ†Ù‡ Ø¥Ø±Ø³Ø§Ù„ Ø¢Ù„Ø§Ù Ø§Ù„Ù€ requests
2. ÙƒÙ„ request ÙŠÙƒÙ„Ù Ù…Ø§Ù„ (OpenAI/Anthropic)
3. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªÙ†Ø²Ø§Ù Ø§Ù„Ù€ budget Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ Ø¯Ù‚Ø§Ø¦Ù‚
4. Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ù„Ø¥Ø³Ø§Ø¡Ø©

---

## ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ£Ø«Ø±Ø©

```
app/api/ai/chat/route.ts
app/api/ai/chat/stream/route.ts
app/api/ai/generate/route.ts
app/api/ai/analyze/route.ts
app/api/questions/auto-answer/route.ts
app/api/reviews/generate-response/route.ts
```

---

## ğŸ› Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ (Ø§Ù„Ù…Ø¹Ø·ÙˆØ¨)

```typescript
// app/api/ai/chat/route.ts
export async function POST(request: Request) {
  // âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ rate limiting!
  // âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ usage tracking!
  // âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ cost control!

  const { message } = await request.json();

  // ÙŠØ³ØªØ¯Ø¹ÙŠ OpenAI Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø¯ÙˆÙ† Ø­Ø¯ÙˆØ¯
  const response = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [{ role: "user", content: message }],
  });

  return Response.json({ response: response.choices[0].message });
}
```

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ø®Ø·ÙŠØ±:**

- GPT-4: ~$0.03 per 1K tokens
- 1000 request Ã— 1000 tokens = $30
- Ù…Ù‡Ø§Ø¬Ù… ÙŠØ±Ø³Ù„ 100,000 request = $3,000+ ÙÙŠ Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©!

---

## âœ… Ø§Ù„Ø­Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨

### Step 1: Ø¥Ù†Ø´Ø§Ø¡ AI Rate Limiter

```typescript
// lib/security/ai-rate-limit.ts
import { Ratelimit } from "@upstash/ratelimit";
import { Redis } from "@upstash/redis";
import { createClient } from "@/lib/supabase/server";

// ============================================================================
// AI-SPECIFIC RATE LIMITING
// ============================================================================
// Different limits for different AI operations based on cost and risk.
// ============================================================================

const redis = new Redis({
  url: process.env.UPSTASH_REDIS_REST_URL!,
  token: process.env.UPSTASH_REDIS_REST_TOKEN!,
});

// Rate limit configurations per AI endpoint type
export const AI_RATE_LIMITS = {
  // Chat: Most common, moderate cost
  chat: {
    free: { requests: 10, window: "1 h" }, // 10 requests/hour for free users
    pro: { requests: 100, window: "1 h" }, // 100 requests/hour for pro
    enterprise: { requests: 1000, window: "1 h" },
  },

  // Generation: Higher cost (longer outputs)
  generate: {
    free: { requests: 5, window: "1 h" },
    pro: { requests: 50, window: "1 h" },
    enterprise: { requests: 500, window: "1 h" },
  },

  // Auto-answer: Very high cost (multiple API calls)
  autoAnswer: {
    free: { requests: 3, window: "1 d" }, // 3 per day for free
    pro: { requests: 50, window: "1 d" },
    enterprise: { requests: 500, window: "1 d" },
  },
} as const;

type AIEndpointType = keyof typeof AI_RATE_LIMITS;
type UserTier = "free" | "pro" | "enterprise";

/**
 * Creates a rate limiter for a specific AI endpoint and user tier.
 */
function createAIRateLimiter(
  endpointType: AIEndpointType,
  userTier: UserTier,
): Ratelimit {
  const config = AI_RATE_LIMITS[endpointType][userTier];

  return new Ratelimit({
    redis,
    limiter: Ratelimit.slidingWindow(
      config.requests,
      config.window as Parameters<typeof Ratelimit.slidingWindow>[1],
    ),
    analytics: true,
    prefix: `nnh:ai:${endpointType}:${userTier}`,
  });
}

export interface AIRateLimitResult {
  success: boolean;
  limit: number;
  remaining: number;
  reset: number;
  retryAfter?: number;
}

/**
 * Checks AI rate limit for a user.
 */
export async function checkAIRateLimit(
  userId: string,
  endpointType: AIEndpointType,
): Promise<AIRateLimitResult> {
  // Get user tier from database
  const supabase = await createClient();
  const { data: profile } = await supabase
    .from("profiles")
    .select("subscription_tier")
    .eq("id", userId)
    .single();

  const userTier: UserTier = profile?.subscription_tier || "free";

  const limiter = createAIRateLimiter(endpointType, userTier);
  const identifier = `${userId}:${endpointType}`;

  try {
    const result = await limiter.limit(identifier);

    return {
      success: result.success,
      limit: result.limit,
      remaining: result.remaining,
      reset: result.reset,
      retryAfter: result.success
        ? undefined
        : Math.ceil((result.reset - Date.now()) / 1000),
    };
  } catch (error) {
    console.error("[AI Rate Limit] Check failed:", error);
    // FAIL CLOSED for AI endpoints (cost protection)
    return {
      success: false,
      limit: 0,
      remaining: 0,
      reset: Date.now() + 60000,
      retryAfter: 60,
    };
  }
}

/**
 * Tracks AI usage for billing and analytics.
 */
export async function trackAIUsage(
  userId: string,
  endpointType: AIEndpointType,
  tokensUsed: number,
  model: string,
): Promise<void> {
  const supabase = await createClient();

  await supabase.from("ai_usage_logs").insert({
    user_id: userId,
    endpoint_type: endpointType,
    tokens_used: tokensUsed,
    model,
    estimated_cost: calculateCost(model, tokensUsed),
    created_at: new Date().toISOString(),
  });
}

/**
 * Calculates estimated cost based on model and tokens.
 */
function calculateCost(model: string, tokens: number): number {
  const costs: Record<string, number> = {
    "gpt-4": 0.00003, // $0.03 per 1K tokens
    "gpt-4-turbo": 0.00001, // $0.01 per 1K tokens
    "gpt-3.5-turbo": 0.000002, // $0.002 per 1K tokens
    "claude-3-opus": 0.000015,
    "claude-3-sonnet": 0.000003,
  };

  const costPerToken = costs[model] || 0.00001;
  return tokens * costPerToken;
}
```

### Step 2: Ø¥Ù†Ø´Ø§Ø¡ AI Auth Middleware

```typescript
// lib/api/with-ai-protection.ts
import { NextResponse } from "next/server";
import { createClient } from "@/lib/supabase/server";
import { checkAIRateLimit, trackAIUsage } from "@/lib/security/ai-rate-limit";

type AIEndpointType = "chat" | "generate" | "autoAnswer";

interface AIProtectionOptions {
  endpointType: AIEndpointType;
  requireAuth?: boolean;
}

/**
 * Higher-order function to protect AI endpoints with:
 * 1. Authentication
 * 2. Rate limiting
 * 3. Usage tracking
 */
export function withAIProtection(
  handler: (request: Request, context: { userId: string }) => Promise<Response>,
  options: AIProtectionOptions,
) {
  return async (request: Request): Promise<Response> => {
    // 1. Authentication
    const supabase = await createClient();
    const {
      data: { user },
      error: authError,
    } = await supabase.auth.getUser();

    if (authError || !user) {
      return NextResponse.json(
        { error: "Authentication required for AI endpoints" },
        { status: 401 },
      );
    }

    // 2. Rate Limiting
    const rateLimit = await checkAIRateLimit(user.id, options.endpointType);

    if (!rateLimit.success) {
      return NextResponse.json(
        {
          error: "AI rate limit exceeded",
          limit: rateLimit.limit,
          remaining: rateLimit.remaining,
          retryAfter: rateLimit.retryAfter,
        },
        {
          status: 429,
          headers: {
            "Retry-After": String(rateLimit.retryAfter || 60),
            "X-RateLimit-Limit": String(rateLimit.limit),
            "X-RateLimit-Remaining": String(rateLimit.remaining),
          },
        },
      );
    }

    // 3. Execute handler
    const response = await handler(request, { userId: user.id });

    // 4. Add rate limit headers to response
    const newResponse = new NextResponse(response.body, response);
    newResponse.headers.set("X-RateLimit-Limit", String(rateLimit.limit));
    newResponse.headers.set(
      "X-RateLimit-Remaining",
      String(rateLimit.remaining - 1),
    );

    return newResponse;
  };
}
```

### Step 3: ØªØ­Ø¯ÙŠØ« AI Routes

```typescript
// app/api/ai/chat/route.ts
import { withAIProtection } from "@/lib/api/with-ai-protection";
import { trackAIUsage } from "@/lib/security/ai-rate-limit";

async function handleChat(
  request: Request,
  { userId }: { userId: string },
): Promise<Response> {
  const { message, conversationHistory } = await request.json();

  // Validate input
  if (!message || typeof message !== "string") {
    return Response.json({ error: "Message is required" }, { status: 400 });
  }

  // Call AI provider
  const response = await openai.chat.completions.create({
    model: "gpt-4-turbo",
    messages: [...conversationHistory, { role: "user", content: message }],
    max_tokens: 1000, // Limit output to control costs
  });

  // Track usage
  const tokensUsed = response.usage?.total_tokens || 0;
  await trackAIUsage(userId, "chat", tokensUsed, "gpt-4-turbo");

  return Response.json({
    response: response.choices[0].message.content,
    usage: {
      tokens: tokensUsed,
      remaining: response.headers?.get("X-RateLimit-Remaining"),
    },
  });
}

export const POST = withAIProtection(handleChat, {
  endpointType: "chat",
});
```

### Step 4: Ø¥Ù†Ø´Ø§Ø¡ Usage Dashboard Component

```typescript
// components/ai/ai-usage-display.tsx
"use client";

import { useEffect, useState } from "react";

interface UsageData {
  used: number;
  limit: number;
  resetAt: string;
}

export function AIUsageDisplay() {
  const [usage, setUsage] = useState<UsageData | null>(null);

  useEffect(() => {
    fetch("/api/ai/usage")
      .then((res) => res.json())
      .then(setUsage);
  }, []);

  if (!usage) return null;

  const percentage = (usage.used / usage.limit) * 100;
  const isLow = percentage > 80;

  return (
    <div className={`p-4 rounded-lg ${isLow ? "bg-red-50" : "bg-gray-50"}`}>
      <h3 className="font-medium">AI Usage</h3>
      <div className="mt-2">
        <div className="flex justify-between text-sm">
          <span>{usage.used} / {usage.limit} requests</span>
          <span>{Math.round(percentage)}%</span>
        </div>
        <div className="mt-1 h-2 bg-gray-200 rounded-full">
          <div
            className={`h-full rounded-full ${isLow ? "bg-red-500" : "bg-blue-500"}`}
            style={{ width: `${percentage}%` }}
          />
        </div>
        <p className="mt-1 text-xs text-gray-500">
          Resets: {new Date(usage.resetAt).toLocaleString()}
        </p>
      </div>
    </div>
  );
}
```

---

## ğŸ” Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ°

### Step 1: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª

```bash
touch lib/security/ai-rate-limit.ts
touch lib/api/with-ai-protection.ts
```

### Step 2: Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Usage ÙÙŠ Database

```sql
-- supabase/migrations/xxx_add_ai_usage_logs.sql
CREATE TABLE ai_usage_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
  endpoint_type TEXT NOT NULL,
  tokens_used INTEGER NOT NULL,
  model TEXT NOT NULL,
  estimated_cost DECIMAL(10, 6),
  created_at TIMESTAMPTZ DEFAULT now()
);

-- Index for querying user usage
CREATE INDEX idx_ai_usage_user_date ON ai_usage_logs(user_id, created_at);

-- RLS
ALTER TABLE ai_usage_logs ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view own usage"
  ON ai_usage_logs FOR SELECT
  USING (auth.uid() = user_id);
```

### Step 3: ØªØ­Ø¯ÙŠØ« Ø¬Ù…ÙŠØ¹ AI Routes

```bash
# Ø§Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ AI routes
find app/api -name "*.ts" | xargs grep -l "openai\|anthropic"
# Ø­Ø¯Ø« ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ù…Ù†Ù‡Ù…
```

---

## âœ… Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„

Ù‚Ø¨Ù„ ÙˆØ¶Ø¹ Ø¹Ù„Ø§Ù…Ø© "Ù…ÙƒØªÙ…Ù„"ØŒ ØªØ£ÙƒØ¯ Ù…Ù†:

- [ ] ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ `ai-rate-limit.ts` Ù…Ø¹ Ø­Ø¯ÙˆØ¯ Ù…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ„ tier
- [ ] ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ `with-ai-protection.ts` HOF
- [ ] Ø¬Ù…ÙŠØ¹ AI routes ØªØ³ØªØ®Ø¯Ù… `withAIProtection`
- [ ] ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ `ai_usage_logs` ÙÙŠ Database
- [ ] Ø§Ù„Ù€ rate limit headers ØªÙØ±Ø³Ù„ ÙÙŠ ÙƒÙ„ response
- [ ] Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ±Ù‰ usage ÙÙŠ Ø§Ù„Ù€ dashboard
- [ ] Free users Ù…Ø­Ø¯ÙˆØ¯ÙŠÙ† Ø¨Ù€ 10 requests/hour
- [ ] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø·Ø§Ø¡ TypeScript: `npx tsc --noEmit`
- [ ] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø·Ø§Ø¡ Lint: `npm run lint`

---

## ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø­Ù„

### Test 1: Rate Limit

```bash
# Ø£Ø±Ø³Ù„ 11 requests ÙƒÙ€ free user
for i in {1..11}; do
  curl -X POST http://localhost:3000/api/ai/chat \
    -H "Content-Type: application/json" \
    -d '{"message": "test"}'
done

# Ø§Ù„Ù€ request Ø§Ù„Ù€ 11 ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ±Ø¬Ø¹ 429
```

### Test 2: Usage Tracking

```sql
-- ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
SELECT * FROM ai_usage_logs WHERE user_id = 'xxx' ORDER BY created_at DESC;
```

---

## ğŸš¨ ØªØ­Ø°ÙŠØ±Ø§Øª Ù…Ù‡Ù…Ø©

### â›” Ù…Ù…Ù†ÙˆØ¹:

- AI endpoints Ø¨Ø¯ÙˆÙ† authentication
- AI endpoints Ø¨Ø¯ÙˆÙ† rate limiting
- Ø¹Ø¯Ù… ØªØªØ¨Ø¹ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
- Ø­Ø¯ÙˆØ¯ Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ù€ free tier

### âœ… Ù…Ø·Ù„ÙˆØ¨:

- Rate limiting per user per endpoint
- Usage tracking for billing
- Different limits per subscription tier
- Cost estimation and alerts

---

**Status:** âœ… COMPLETED
**Blocked By:** None
**Blocks:** None

---

**Ù‡Ø°Ø§ Ø¥ØµÙ„Ø§Ø­ Ø­Ø±Ø¬ Ù„Ù„ØªÙƒÙ„ÙØ©. Ø¨Ø¯ÙˆÙ†Ù‡ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªÙ†Ø²Ø§Ù Ø§Ù„Ù€ budget Ø¨Ø§Ù„ÙƒØ§Ù…Ù„!** ğŸ’°
